{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ PoC AI Pool 2025 ~\n",
    "- ## Day 3: Deep Learning\n",
    "    - ### Module 2: Convolutional Neural Network\n",
    "-----------\n",
    "\n",
    "## Minst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you've arrived here ! You now understand key concepts of neural networks and how they are trained, but you haven't really created one yet...\n",
    "Don't worry this task will guide you in recreating a neural network trained to detect any handwritten digit on a 28 by 28 pixel image !\n",
    "\n",
    "Your will start by setup the dataset, your model and at the end, play with it ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just import the necessary libraries\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#For the model don't forget\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part - 1 Prepare the data \n",
    "\n",
    "before actually create a neural network we need to preparate our data that we will fit to your model,\n",
    "\n",
    "remember ***THE MOST important in machine learning is the quality of the data*** and not really the model....\n",
    "\n",
    "your goal here is to specify how we want the data, this can be process by initialise a data and transform it in a [tensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) and normalize it if you want. you can check the doc of transform [here](https://pytorch.org/vision/0.9/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train dataset : 60000\n",
      "Len test  dataset : 10000\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),           # Rotation aléatoire ±10°\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Translation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalisation MNIST\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "eval_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "print(f\"Len train dataset : {len(train_set)}\")\n",
    "print(f\"Len test  dataset : {len(eval_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will say why whe created two dataset ? \n",
    "\n",
    "It's because one will be for the training of the model and the other for evaluate this one by passing data he never seen, to see if the model didn't overfit the data.\n",
    "\n",
    "To understand what's inside this code you can try below to visualise some of the examples !\n",
    "\n",
    "***Don't hesitate to change the NUMBER_OF_ELEMENTS enum to see mutliples examples or no***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.42421296..2.8214867].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels : 0; 3; 5; 6;\n"
     ]
    }
   ],
   "source": [
    "# Visualisation of some element of the dataset you can change the number if you want\n",
    "NUMBER_OF_ELEMENTS = 4\n",
    "\n",
    "def imshow(img):\n",
    "    # img = img * 0.5 + 0.5  # Denormalisation if you have normalised the data\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "train_loader_vis = torch.utils.data.DataLoader(train_set, batch_size=NUMBER_OF_ELEMENTS, shuffle=True)\n",
    "\n",
    "# Random image \n",
    "dataiter = iter(train_loader_vis)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Labels :', ' '.join(f'{labels[j].item()};' for j in range(NUMBER_OF_ELEMENTS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at different attributes like the number of images in the dataset, the size of each image or the label of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           1.8032, -0.0933,  1.6887,  2.8215,  2.7197,  1.1923, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3860, -0.1951, -0.1951, -0.1951,  1.1795,  1.3068,\n",
      "           2.4396,  1.7650,  2.7960,  2.6560,  2.0578,  0.3904, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           1.5359,  1.7396,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           0.7595,  0.6195,  0.6195,  0.2886,  0.0722, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.0424,  0.0340,  0.7722,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7706,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  0.1995,  2.6051,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.0960,  1.8923,  2.7197,  2.6433,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.1951,  2.3633,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.1851, -0.2842, -0.4242,  0.1231,  1.5359,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.5940,  1.5614,  0.9377,\n",
      "           1.5359,  2.7960,  0.7213,  0.7213, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2460,\n",
      "          -0.4242,  1.3450,  2.7960,  1.9942, -0.3988, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.2842,  1.9942,  2.7960,  0.4668, -0.4242, -0.4242,\n",
      "          -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.0213,  2.6433,  2.4396,  1.6123,  0.9504,\n",
      "          -0.4115, -0.1060, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.6068,  2.6306,  2.7960,  2.7960,\n",
      "           1.0904,  1.4850, -0.0806, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.1486,  1.9432,  2.7960,\n",
      "           2.7960,  2.7960,  1.9560, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2206,  0.7595,\n",
      "           2.7833,  2.7960,  2.7451,  0.3904, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.7960,  2.7960,  2.2105, -0.3988, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.1613,  1.2305,\n",
      "           1.9051,  2.7960,  2.7578,  1.8923, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,  2.4906,  2.7960,\n",
      "           2.7960,  2.1342,  0.5686, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960, -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.2869,  2.7960,  2.7960,  2.7960,  2.7960,  2.0960,\n",
      "           0.6068, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1315,\n",
      "           0.4159,  2.7960,  2.7960,  2.7960,  2.0578,  0.5940, -0.3097,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.1951,  1.7523,  2.3633,\n",
      "           2.7960,  2.7960,  2.6815,  1.2686, -0.2842, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242,  0.2758,  1.7650,  2.4524,  2.7960,  2.7960,\n",
      "           2.7960,  1.2559, -0.2206, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,  2.7960,\n",
      "           2.2742, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "------------------------------------------------------------\n",
      "image shape : torch.Size([1, 28, 28])\n",
      "label : 5\n"
     ]
    }
   ],
   "source": [
    "image, label = train_set[0]\n",
    "\n",
    "print(\"image :\", image) # pixels value if you want to see the matrix\n",
    "print(\"-\"*60)\n",
    "print(\"image shape :\", image.shape) # pixels value\n",
    "print(\"label :\", label) # Number represented in the image \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have images **28 pixels high and 28 pixels wide**, with **one channel** (grayscale !).\n",
    "\n",
    "These images represent a number from 0 to 9, we have **10 different labels** (or 10 different possible output).\\\n",
    "The first picture represents a 5, therefore its label is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch-Size\n",
    "\n",
    "Did you remember when we talk about batch and parallelization of multiple example with torch ? This is very important here !\n",
    "\n",
    "**60,000** is a lot of images to process one by one, to make it easier for our model to process this data while training we are going to use ``batch_size``.\n",
    "\n",
    "for one who forget , ``batch_size`` is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In other words, before calculating the error and apply backpropagation after each image, if our batch size is 64 we will go through 64 images before doing it. **This improves the learning of our AI** by **applying the backpropagation on the error average.**\n",
    "\n",
    "As in the previous notebook we will use a [**``dataloader``**](https://pytorch.org/docs/stable/data.html), this time we don't need to redefine a ``Dataset`` class since we are using a ``builtin`` dataset in ``torchvision``.\n",
    "\n",
    "Remember to specify that you use the ``train_set`` and you want a ``batch_size`` of ``64`` and also ``shuffle`` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape : torch.Size([128, 1, 28, 28])\n",
      "labels shape : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "#TODO : Define the batch size\n",
    "import torch.utils.data.dataloader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# assert len(train_loader) == 938, \"Your train loader is not well implemented, remember that the batch size is 64\"\n",
    "\n",
    "batch = next(iter(train_loader)) # obtain the first batch\n",
    "images, labels = batch\n",
    "print(\"image shape :\", images.shape)\n",
    "print(\"labels shape :\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `938` lots containing `64` images each (and their equivalent labels).\\\n",
    "This will **drastically decrease our training time** because with one backward propagation, 64 images are processed.\n",
    "\n",
    "\n",
    "> Pytorch is built to be used with batch, it is thus quite simple to implement it in our code. \n",
    "\n",
    "*you can try after to change your batch and see the difference in the learning (remove the assert for test it)* !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Also load the test set with the same batch_size...\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(eval_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# assert len(eval_loader) == 157, \"Your eval loader is not well implemented\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model!\n",
    "\n",
    "And your moment has arrived!\n",
    "\n",
    "I’m sure you’ve been eagerly anticipating this step, and now you’re ready to build your very first real neural network, complete with a more complex architecture.\n",
    "\n",
    "A quick tip for working with PyTorch: today’s task is a classification problem, as we’ve defined specific output labels. For this, we’ll be using the **[cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)** loss function. (Remember, yesterday you used the **[binary cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)** loss with logistic regression, since the output was restricted to just 0 or 1.)\n",
    "\n",
    "*Don't hesistate to jump at the end of the torch introduction as helping you for initialize the model and train it !*\n",
    "\n",
    "IF you encounter difficulties to create your model, at the end of this notebook there is a pseudo code of the architecture as to help you to create the model, but try to do it alone ! (with everything you see before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Deuxième bloc avec skip connection\n",
    "        self.conv2a = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2a = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Troisième bloc\n",
    "        self.conv3a = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3a = nn.BatchNorm2d(128)\n",
    "        self.conv3b = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3b = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout_conv = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Fully connected\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )        \n",
    "        # Device choice \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(f\"Device : {self.device}\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)\n",
    "        \n",
    "        # Bloc 2 (résiduel)\n",
    "        identity = x\n",
    "        x = self.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = self.bn2b(self.conv2b(x))\n",
    "        # Skip connection (adapter les dimensions si nécessaire)\n",
    "        if identity.shape[1] != x.shape[1]:\n",
    "            identity = nn.Conv2d(32, 64, kernel_size=1).to(x.device)(identity)\n",
    "        x = self.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)\n",
    "        \n",
    "        # Bloc 3\n",
    "        x = self.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = self.relu(self.bn3b(self.conv3b(x)))\n",
    "        \n",
    "        # FC layers\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = self.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def train_model(self, epochs, train_loader):\n",
    "        self.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            total_batches = 0\n",
    "\n",
    "            for i, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = self.loss(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "                if (i + 1) % 8 == 0 or (i + 1) == len(train_loader):\n",
    "                    print(f\"\\rEpochs {epoch + 1}/{epochs} | Lot {i + 1}/{len(train_loader)} | Loss : {loss.item():.4f}\", end='')\n",
    "\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Epochs {epoch + 1}/{epochs} finish | Average Loss : {avg_loss:.4f} | Time : {epoch_time:.2f} seconds\")\n",
    "            print(\"-\" * 60)\n",
    "            self.scheduler.step(avg_loss)\n",
    "\n",
    "        model_path = \"mnist_model.pth\"\n",
    "        print('Training finished, saving model to :', model_path)\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    def eval_model(self, test_loader):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on {total} images is : {100 * correct / total:.2f}%')\n",
    "\n",
    "    def load_weights(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path, weights_only=True, map_location=self.device))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done ! you need now to initialise your model by simple call your python class, \n",
    "\n",
    "It permits that if you want to restart the training with random weights, you can restart this cell. Otherwise, the training if (you restart it) will continue from the **`last loss value`** and the **`last weight`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugomath/epitech-projects/poc_ia/pocIA/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_model = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can now test your model by simply call the eval function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on 10000 images is : 9.58%\n"
     ]
    }
   ],
   "source": [
    "my_model.eval_model(eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’d like to retrain and check for better results, simply re-run the training cell or initialize a new model to start fresh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with your model !\n",
    "\n",
    "Now it's time to test your own model! **Please paste your model architecture** (*`__init__`* and *`forward`* methods) into the file [model.py](model.py), and run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "after this break, you have two option : \n",
    "\n",
    "- ***2.2 - Cifar*** -> try to implemente an really complex architecture called VAE-GAN for another task \n",
    "\n",
    "- ***3.1 - My torch*** -> try to recreate some function of torch, to really understand how this is work (it my be help you for creating a VAE-GAN architecture :))\n",
    "\n",
    "choose one ! *(you can do both also if you finish in advance)*\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : mps\n",
      "Epochs 1/10 | Lot 469/469 | Loss : 0.0726\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 1/10 finish | Average Loss : 0.3265 | Time : 10.26 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 2/10 | Lot 469/469 | Loss : 0.1057\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 2/10 finish | Average Loss : 0.1051 | Time : 8.04 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 3/10 | Lot 469/469 | Loss : 0.0864\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 3/10 finish | Average Loss : 0.0803 | Time : 8.07 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 4/10 | Lot 469/469 | Loss : 0.0664\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 4/10 finish | Average Loss : 0.0688 | Time : 8.11 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 5/10 | Lot 469/469 | Loss : 0.2299\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 5/10 finish | Average Loss : 0.0624 | Time : 8.10 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 6/10 | Lot 469/469 | Loss : 0.1051\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 6/10 finish | Average Loss : 0.0563 | Time : 8.25 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 7/10 | Lot 469/469 | Loss : 0.0756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 7/10 finish | Average Loss : 0.0530 | Time : 8.10 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 8/10 | Lot 469/469 | Loss : 0.0284\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 8/10 finish | Average Loss : 0.0498 | Time : 8.12 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 9/10 | Lot 469/469 | Loss : 0.1355\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 9/10 finish | Average Loss : 0.0465 | Time : 8.11 seconds\n",
      "------------------------------------------------------------\n",
      "Epochs 10/10 | Lot 469/469 | Loss : 0.0627\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epochs 10/10 finish | Average Loss : 0.0448 | Time : 8.19 seconds\n",
      "------------------------------------------------------------\n",
      "Training finished, saving model to : mnist_model.pth\n",
      "Accuracy of the model on 10000 images is : 99.54%\n"
     ]
    }
   ],
   "source": [
    "my_model = MNISTModel()\n",
    "#TODO: define number of epochs\n",
    "EPOCHS = 10\n",
    "\n",
    "my_model.train_model(EPOCHS, train_loader)\n",
    "my_model.eval_model(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocIA (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
